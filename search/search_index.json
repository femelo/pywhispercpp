{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PyWhisperCpp API Reference","text":""},{"location":"#pywhispercpp.model","title":"pywhispercpp.model","text":"<p>This module contains a simple Python API on-top of the C-style whisper.cpp API.</p>"},{"location":"#pywhispercpp.model.Segment","title":"Segment","text":"<pre><code>Segment(t0, t1, text, probability=np.nan)\n</code></pre> <p>A small class representing a transcription segment</p> <p>Parameters:</p> <ul> <li> <code>t0</code>               (<code>int</code>)           \u2013            <p>start time</p> </li> <li> <code>t1</code>               (<code>int</code>)           \u2013            <p>end time</p> </li> <li> <code>text</code>               (<code>str</code>)           \u2013            <p>text</p> </li> <li> <code>probability</code>               (<code>float</code>, default:                   <code>nan</code> )           \u2013            <p>Confidence score for the segment, computed as the geometric mean of the token probabilities for the segment (NaN if not calculated). This makes it interpretable as a probability in [0, 1].</p> </li> </ul> Source code in <code>pywhispercpp/model.py</code> <pre><code>def __init__(self, t0: int, t1: int, text: str, probability: float = np.nan):\n    \"\"\"\n    :param t0: start time\n    :param t1: end time\n    :param text: text\n    :param probability: Confidence score for the segment, computed as the geometric mean of\n        the token probabilities for the segment (NaN if not calculated).\n        This makes it interpretable as a probability in [0, 1].\n    \"\"\"\n    self.t0 = t0\n    self.t1 = t1\n    self.text = text\n    self.probability = probability\n</code></pre>"},{"location":"#pywhispercpp.model.Model","title":"Model","text":"<pre><code>Model(\n    model=\"tiny\",\n    models_dir=None,\n    params_sampling_strategy=0,\n    redirect_whispercpp_logs_to=False,\n    **params\n)\n</code></pre> <p>This classes defines a Whisper.cpp model.</p> <p>Example usage. <pre><code>model = Model('base.en', n_threads=6)\nsegments = model.transcribe('file.mp3')\nfor segment in segments:\n    print(segment.text)\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>str</code>, default:                   <code>'tiny'</code> )           \u2013            <p>The name of the model, one of the AVAILABLE_MODELS, (default to <code>tiny</code>), or a direct path to a <code>ggml</code> model.</p> </li> <li> <code>models_dir</code>               (<code>str</code>, default:                   <code>None</code> )           \u2013            <p>The directory where the models are stored, or where they will be downloaded if they don't exist, default to MODELS_DIR <li> <code>params_sampling_strategy</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>0 -&gt; GREEDY, else BEAM_SEARCH</p> </li> <li> <code>redirect_whispercpp_logs_to</code>               (<code>Union[bool, TextIO, str, None]</code>, default:                   <code>False</code> )           \u2013            <p>where to redirect the whisper.cpp logs, default to False (no redirection), accepts str file path, sys.stdout, sys.stderr, or use None to redirect to devnull</p> </li> <li> <code>params</code>           \u2013            <p>keyword arguments for different whisper.cpp parameters, see PARAMS_SCHEMA</p> </li> Source code in <code>pywhispercpp/model.py</code> <pre><code>def __init__(self,\n             model: str = 'tiny',\n             models_dir: str = None,\n             params_sampling_strategy: int = 0,\n             redirect_whispercpp_logs_to: Union[bool, TextIO, str, None] = False,\n             **params):\n    \"\"\"\n    :param model: The name of the model, one of the [AVAILABLE_MODELS](/pywhispercpp/#pywhispercpp.constants.AVAILABLE_MODELS),\n                    (default to `tiny`), or a direct path to a `ggml` model.\n    :param models_dir: The directory where the models are stored, or where they will be downloaded if they don't\n                        exist, default to [MODELS_DIR](/pywhispercpp/#pywhispercpp.constants.MODELS_DIR) &lt;user_data_dir/pywhsipercpp/models&gt;\n    :param params_sampling_strategy: 0 -&gt; GREEDY, else BEAM_SEARCH\n    :param redirect_whispercpp_logs_to: where to redirect the whisper.cpp logs, default to False (no redirection), accepts str file path, sys.stdout, sys.stderr, or use None to redirect to devnull\n    :param params: keyword arguments for different whisper.cpp parameters,\n                    see [PARAMS_SCHEMA](/pywhispercpp/#pywhispercpp.constants.PARAMS_SCHEMA)\n    \"\"\"\n    if Path(model).is_file():\n        self.model_path = model\n    else:\n        self.model_path = utils.download_model(model, models_dir)\n    self._ctx = None\n    self._sampling_strategy = pw.whisper_sampling_strategy.WHISPER_SAMPLING_GREEDY if params_sampling_strategy == 0 else \\\n        pw.whisper_sampling_strategy.WHISPER_SAMPLING_BEAM_SEARCH\n    self._params = pw.whisper_full_default_params(self._sampling_strategy)\n    # assign params\n    self._set_params(params)\n    self.redirect_whispercpp_logs_to = redirect_whispercpp_logs_to\n    # init the model\n    self._init_model()\n</code></pre>"},{"location":"#pywhispercpp.model.Model.transcribe","title":"transcribe","text":"<pre><code>transcribe(\n    media,\n    n_processors=None,\n    new_segment_callback=None,\n    **params\n)\n</code></pre> <p>Transcribes the media provided as input and returns list of <code>Segment</code> objects. Accepts a media_file path (audio/video) or a raw numpy array.</p> <p>Parameters:</p> <ul> <li> <code>media</code>               (<code>Union[str, ndarray]</code>)           \u2013            <p>Media file path or a numpy array</p> </li> <li> <code>n_processors</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>if not None, it will run the transcription on multiple processes binding to whisper.cpp/whisper_full_parallel &gt; Split the input audio in chunks and process each chunk separately using whisper_full()</p> </li> <li> <code>new_segment_callback</code>               (<code>Callable[[Segment], None]</code>, default:                   <code>None</code> )           \u2013            <p>callback function that will be called when a new segment is generated</p> </li> <li> <code>params</code>           \u2013            <p>keyword arguments for different whisper.cpp parameters, see ::: constants.PARAMS_SCHEMA</p> </li> <li> <code>extract_probability</code>           \u2013            <p>If True, calculates the geometric mean of token probabilities for each segment, providing a confidence score interpretable as a probability in [0, 1].</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>List[Segment]</code>           \u2013            <p>List of transcription segments</p> </li> </ul> Source code in <code>pywhispercpp/model.py</code> <pre><code>def transcribe(self,\n               media: Union[str, np.ndarray],\n               n_processors: int = None,\n               new_segment_callback: Callable[[Segment], None] = None,\n               **params) -&gt; List[Segment]:\n    \"\"\"\n    Transcribes the media provided as input and returns list of `Segment` objects.\n    Accepts a media_file path (audio/video) or a raw numpy array.\n\n    :param media: Media file path or a numpy array\n    :param n_processors: if not None, it will run the transcription on multiple processes\n                         binding to whisper.cpp/whisper_full_parallel\n                         &gt; Split the input audio in chunks and process each chunk separately using whisper_full()\n    :param new_segment_callback: callback function that will be called when a new segment is generated\n    :param params: keyword arguments for different whisper.cpp parameters, see ::: constants.PARAMS_SCHEMA\n    :param extract_probability: If True, calculates the geometric mean of token probabilities for each segment,\n        providing a confidence score interpretable as a probability in [0, 1].\n    :return: List of transcription segments\n    \"\"\"\n    if type(media) is np.ndarray:\n        audio = media\n    else:\n        if not Path(media).exists():\n            raise FileNotFoundError(media)\n        audio = self._load_audio(media)\n\n    # Handle extract_probability parameter\n    self.extract_probability = params.pop('extract_probability', False)\n\n    # update params if any\n    self._set_params(params)\n\n    # setting up callback\n    if new_segment_callback:\n        Model._new_segment_callback = new_segment_callback\n        pw.assign_new_segment_callback(self._params, Model.__call_new_segment_callback)\n\n    # run inference\n    start_time = time()\n    logger.info(\"Transcribing ...\")\n    res = self._transcribe(audio, n_processors=n_processors)\n    end_time = time()\n    logger.info(f\"Inference time: {end_time - start_time:.3f} s\")\n    return res\n</code></pre>"},{"location":"#pywhispercpp.model.Model.get_params","title":"get_params","text":"<pre><code>get_params()\n</code></pre> <p>Returns a <code>dict</code> representation of the actual params</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>params dict</p> </li> </ul> Source code in <code>pywhispercpp/model.py</code> <pre><code>def get_params(self) -&gt; dict:\n    \"\"\"\n    Returns a `dict` representation of the actual params\n\n    :return: params dict\n    \"\"\"\n    res = {}\n    for param in dir(self._params):\n        if param.startswith('__'):\n            continue\n        try:\n            res[param] = getattr(self._params, param)\n        except Exception:\n            # ignore callback functions\n            continue\n    return res\n</code></pre>"},{"location":"#pywhispercpp.model.Model.get_params_schema","title":"get_params_schema  <code>staticmethod</code>","text":"<pre><code>get_params_schema()\n</code></pre> <p>A simple link to ::: constants.PARAMS_SCHEMA</p> <p>Returns:</p> <ul> <li> <code>dict</code>           \u2013            <p>dict of params schema</p> </li> </ul> Source code in <code>pywhispercpp/model.py</code> <pre><code>@staticmethod\ndef get_params_schema() -&gt; dict:\n    \"\"\"\n    A simple link to ::: constants.PARAMS_SCHEMA\n    :return: dict of params schema\n    \"\"\"\n    return constants.PARAMS_SCHEMA\n</code></pre>"},{"location":"#pywhispercpp.model.Model.lang_max_id","title":"lang_max_id  <code>staticmethod</code>","text":"<pre><code>lang_max_id()\n</code></pre> <p>Returns number of supported languages. Direct binding to whisper.cpp/lang_max_id</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            </li> </ul> Source code in <code>pywhispercpp/model.py</code> <pre><code>@staticmethod\ndef lang_max_id() -&gt; int:\n    \"\"\"\n    Returns number of supported languages.\n    Direct binding to whisper.cpp/lang_max_id\n    :return:\n    \"\"\"\n    return pw.whisper_lang_max_id()\n</code></pre>"},{"location":"#pywhispercpp.model.Model.print_timings","title":"print_timings","text":"<pre><code>print_timings()\n</code></pre> <p>Direct binding to whisper.cpp/whisper_print_timings</p> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>pywhispercpp/model.py</code> <pre><code>def print_timings(self) -&gt; None:\n    \"\"\"\n    Direct binding to whisper.cpp/whisper_print_timings\n\n    :return: None\n    \"\"\"\n    pw.whisper_print_timings(self._ctx)\n</code></pre>"},{"location":"#pywhispercpp.model.Model.system_info","title":"system_info  <code>staticmethod</code>","text":"<pre><code>system_info()\n</code></pre> <p>Direct binding to whisper.cpp/whisper_print_system_info</p> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>pywhispercpp/model.py</code> <pre><code>@staticmethod\ndef system_info() -&gt; None:\n    \"\"\"\n    Direct binding to whisper.cpp/whisper_print_system_info\n\n    :return: None\n    \"\"\"\n    return pw.whisper_print_system_info()\n</code></pre>"},{"location":"#pywhispercpp.model.Model.available_languages","title":"available_languages  <code>staticmethod</code>","text":"<pre><code>available_languages()\n</code></pre> <p>Returns a list of supported language codes</p> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>list of supported language codes</p> </li> </ul> Source code in <code>pywhispercpp/model.py</code> <pre><code>@staticmethod\ndef available_languages() -&gt; list[str]:\n    \"\"\"\n    Returns a list of supported language codes\n\n    :return: list of supported language codes\n    \"\"\"\n    n = pw.whisper_lang_max_id()\n    res = []\n    for i in range(n+1):\n        res.append(pw.whisper_lang_str(i))\n    return res\n</code></pre>"},{"location":"#pywhispercpp.model.Model.auto_detect_language","title":"auto_detect_language","text":"<pre><code>auto_detect_language(media, offset_ms=0, n_threads=4)\n</code></pre> <p>Automatic language detection using whisper.cpp/whisper_pcm_to_mel and whisper.cpp/whisper_lang_auto_detect</p> <p>Parameters:</p> <ul> <li> <code>media</code>               (<code>Union[str, ndarray]</code>)           \u2013            <p>Media file path or a numpy array</p> </li> <li> <code>offset_ms</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>offset in milliseconds</p> </li> <li> <code>n_threads</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>number of threads to use</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[Tuple[str, float32], dict[str, float32]]</code>           \u2013            <p>((detected_language, probability), probabilities for all languages)</p> </li> </ul> Source code in <code>pywhispercpp/model.py</code> <pre><code>def auto_detect_language(self,  media: Union[str, np.ndarray], offset_ms: int = 0, n_threads: int = 4) -&gt; Tuple[Tuple[str, np.float32], dict[str, np.float32]]:\n    \"\"\"\n    Automatic language detection using whisper.cpp/whisper_pcm_to_mel and whisper.cpp/whisper_lang_auto_detect\n\n    :param media: Media file path or a numpy array\n    :param offset_ms: offset in milliseconds\n    :param n_threads: number of threads to use\n    :return: ((detected_language, probability), probabilities for all languages)\n    \"\"\"\n    if type(media) is np.ndarray:\n        audio = media\n    else:\n        if not Path(media).exists():\n            raise FileNotFoundError(media)\n        audio = self._load_audio(media)\n\n    pw.whisper_pcm_to_mel(self._ctx, audio, len(audio), n_threads)\n    lang_max_id = self.lang_max_id()\n    probs = np.zeros(lang_max_id, dtype=np.float32)\n    auto_detect = pw.whisper_lang_auto_detect(self._ctx, offset_ms, n_threads, probs)\n    langs = self.available_languages()\n    lang_probs = {langs[i]: probs[i] for i in range(lang_max_id)}\n    return (langs[auto_detect], probs[auto_detect]), lang_probs\n</code></pre>"},{"location":"#pywhispercpp.constants","title":"pywhispercpp.constants","text":"<p>Constants</p>"},{"location":"#pywhispercpp.constants.WHISPER_SAMPLE_RATE","title":"WHISPER_SAMPLE_RATE  <code>module-attribute</code>","text":"<pre><code>WHISPER_SAMPLE_RATE = WHISPER_SAMPLE_RATE\n</code></pre>"},{"location":"#pywhispercpp.constants.MODELS_BASE_URL","title":"MODELS_BASE_URL  <code>module-attribute</code>","text":"<pre><code>MODELS_BASE_URL = (\n    \"https://huggingface.co/ggerganov/whisper.cpp\"\n)\n</code></pre>"},{"location":"#pywhispercpp.constants.MODELS_PREFIX_URL","title":"MODELS_PREFIX_URL  <code>module-attribute</code>","text":"<pre><code>MODELS_PREFIX_URL = 'resolve/main/ggml'\n</code></pre>"},{"location":"#pywhispercpp.constants.PACKAGE_NAME","title":"PACKAGE_NAME  <code>module-attribute</code>","text":"<pre><code>PACKAGE_NAME = 'pywhispercpp'\n</code></pre>"},{"location":"#pywhispercpp.constants.MODELS_DIR","title":"MODELS_DIR  <code>module-attribute</code>","text":"<pre><code>MODELS_DIR = Path(user_data_dir(PACKAGE_NAME)) / 'models'\n</code></pre>"},{"location":"#pywhispercpp.constants.AVAILABLE_MODELS","title":"AVAILABLE_MODELS  <code>module-attribute</code>","text":"<pre><code>AVAILABLE_MODELS = [\n    \"base\",\n    \"base-q5_1\",\n    \"base-q8_0\",\n    \"base.en\",\n    \"base.en-q5_1\",\n    \"base.en-q8_0\",\n    \"large-v1\",\n    \"large-v2\",\n    \"large-v2-q5_0\",\n    \"large-v2-q8_0\",\n    \"large-v3\",\n    \"large-v3-q5_0\",\n    \"large-v3-turbo\",\n    \"large-v3-turbo-q5_0\",\n    \"large-v3-turbo-q8_0\",\n    \"medium\",\n    \"medium-q5_0\",\n    \"medium-q8_0\",\n    \"medium.en\",\n    \"medium.en-q5_0\",\n    \"medium.en-q8_0\",\n    \"small\",\n    \"small-q5_1\",\n    \"small-q8_0\",\n    \"small.en\",\n    \"small.en-q5_1\",\n    \"small.en-q8_0\",\n    \"tiny\",\n    \"tiny-q5_1\",\n    \"tiny-q8_0\",\n    \"tiny.en\",\n    \"tiny.en-q5_1\",\n    \"tiny.en-q8_0\",\n]\n</code></pre>"},{"location":"#pywhispercpp.constants.PARAMS_SCHEMA","title":"PARAMS_SCHEMA  <code>module-attribute</code>","text":"<pre><code>PARAMS_SCHEMA = {\n    \"n_threads\": {\n        \"type\": int,\n        \"description\": \"Number of threads to allocate for the inferencedefault to min(4, available hardware_concurrency)\",\n        \"options\": None,\n        \"default\": None,\n    },\n    \"n_max_text_ctx\": {\n        \"type\": int,\n        \"description\": \"max tokens to use from past text as prompt for the decoder\",\n        \"options\": None,\n        \"default\": 16384,\n    },\n    \"offset_ms\": {\n        \"type\": int,\n        \"description\": \"start offset in ms\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"duration_ms\": {\n        \"type\": int,\n        \"description\": \"audio duration to process in ms\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"translate\": {\n        \"type\": bool,\n        \"description\": \"whether to translate the audio to English\",\n        \"options\": None,\n        \"default\": False,\n    },\n    \"no_context\": {\n        \"type\": bool,\n        \"description\": \"do not use past transcription (if any) as initial prompt for the decoder\",\n        \"options\": None,\n        \"default\": False,\n    },\n    \"single_segment\": {\n        \"type\": bool,\n        \"description\": \"force single segment output (useful for streaming)\",\n        \"options\": None,\n        \"default\": False,\n    },\n    \"print_special\": {\n        \"type\": bool,\n        \"description\": \"print special tokens (e.g. &lt;SOT&gt;, &lt;EOT&gt;, &lt;BEG&gt;, etc.)\",\n        \"options\": None,\n        \"default\": False,\n    },\n    \"print_progress\": {\n        \"type\": bool,\n        \"description\": \"print progress information\",\n        \"options\": None,\n        \"default\": True,\n    },\n    \"print_realtime\": {\n        \"type\": bool,\n        \"description\": \"print results from within whisper.cpp (avoid it, use callback instead)\",\n        \"options\": None,\n        \"default\": False,\n    },\n    \"print_timestamps\": {\n        \"type\": bool,\n        \"description\": \"print timestamps for each text segment when printing realtime\",\n        \"options\": None,\n        \"default\": True,\n    },\n    \"token_timestamps\": {\n        \"type\": bool,\n        \"description\": \"enable token-level timestamps\",\n        \"options\": None,\n        \"default\": False,\n    },\n    \"thold_pt\": {\n        \"type\": float,\n        \"description\": \"timestamp token probability threshold (~0.01)\",\n        \"options\": None,\n        \"default\": 0.01,\n    },\n    \"thold_ptsum\": {\n        \"type\": float,\n        \"description\": \"timestamp token sum probability threshold (~0.01)\",\n        \"options\": None,\n        \"default\": 0.01,\n    },\n    \"max_len\": {\n        \"type\": int,\n        \"description\": \"max segment length in characters, note: token_timestamps needs to be set to True for this to work\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"split_on_word\": {\n        \"type\": bool,\n        \"description\": \"split on word rather than on token (when used with max_len)\",\n        \"options\": None,\n        \"default\": False,\n    },\n    \"max_tokens\": {\n        \"type\": int,\n        \"description\": \"max tokens per segment (0 = no limit)\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"audio_ctx\": {\n        \"type\": int,\n        \"description\": \"overwrite the audio context size (0 = use default)\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"initial_prompt\": {\n        \"type\": str,\n        \"description\": \"Initial prompt, these are prepended to any existing text context from a previous call\",\n        \"options\": None,\n        \"default\": None,\n    },\n    \"prompt_tokens\": {\n        \"type\": Tuple,\n        \"description\": \"tokens to provide to the whisper decoder as initial prompt\",\n        \"options\": None,\n        \"default\": None,\n    },\n    \"prompt_n_tokens\": {\n        \"type\": int,\n        \"description\": \"tokens to provide to the whisper decoder as initial prompt\",\n        \"options\": None,\n        \"default\": 0,\n    },\n    \"language\": {\n        \"type\": str,\n        \"description\": 'for auto-detection, set to None, \"\" or \"auto\"',\n        \"options\": None,\n        \"default\": \"\",\n    },\n    \"suppress_blank\": {\n        \"type\": bool,\n        \"description\": \"common decoding parameters\",\n        \"options\": None,\n        \"default\": True,\n    },\n    \"suppress_non_speech_tokens\": {\n        \"type\": bool,\n        \"description\": \"common decoding parameters\",\n        \"options\": None,\n        \"default\": False,\n    },\n    \"temperature\": {\n        \"type\": float,\n        \"description\": \"initial decoding temperature\",\n        \"options\": None,\n        \"default\": 0.0,\n    },\n    \"max_initial_ts\": {\n        \"type\": float,\n        \"description\": \"max_initial_ts\",\n        \"options\": None,\n        \"default\": 1.0,\n    },\n    \"length_penalty\": {\n        \"type\": float,\n        \"description\": \"length_penalty\",\n        \"options\": None,\n        \"default\": -1.0,\n    },\n    \"temperature_inc\": {\n        \"type\": float,\n        \"description\": \"temperature_inc\",\n        \"options\": None,\n        \"default\": 0.2,\n    },\n    \"entropy_thold\": {\n        \"type\": float,\n        \"description\": 'similar to OpenAI\\'s \"compression_ratio_threshold\"',\n        \"options\": None,\n        \"default\": 2.4,\n    },\n    \"logprob_thold\": {\n        \"type\": float,\n        \"description\": \"logprob_thold\",\n        \"options\": None,\n        \"default\": -1.0,\n    },\n    \"no_speech_thold\": {\n        \"type\": float,\n        \"description\": \"no_speech_thold\",\n        \"options\": None,\n        \"default\": 0.6,\n    },\n    \"greedy\": {\n        \"type\": dict,\n        \"description\": \"greedy\",\n        \"options\": None,\n        \"default\": {\"best_of\": -1},\n    },\n    \"beam_search\": {\n        \"type\": dict,\n        \"description\": \"beam_search\",\n        \"options\": None,\n        \"default\": {\"beam_size\": -1, \"patience\": -1.0},\n    },\n    \"extract_probability\": {\n        \"type\": bool,\n        \"description\": \"calculate the geometric mean of token probabilities for each segment.\",\n        \"options\": None,\n        \"default\": True,\n    },\n}\n</code></pre>"},{"location":"#pywhispercpp.utils","title":"pywhispercpp.utils","text":"<p>Helper functions</p>"},{"location":"#pywhispercpp.utils.download_model","title":"download_model","text":"<pre><code>download_model(\n    model_name, download_dir=None, chunk_size=1024\n)\n</code></pre> <p>Helper function to download the <code>ggml</code> models</p> <p>Parameters:</p> <ul> <li> <code>model_name</code>               (<code>str</code>)           \u2013            <p>name of the model, one of ::: constants.AVAILABLE_MODELS</p> </li> <li> <code>download_dir</code>           \u2013            <p>Where to store the models</p> </li> <li> <code>chunk_size</code>           \u2013            <p>size of the download chunk</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Absolute path of the downloaded model</p> </li> </ul> Source code in <code>pywhispercpp/utils.py</code> <pre><code>def download_model(model_name: str, download_dir=None, chunk_size=1024) -&gt; str:\n    \"\"\"\n    Helper function to download the `ggml` models\n    :param model_name: name of the model, one of ::: constants.AVAILABLE_MODELS\n    :param download_dir: Where to store the models\n    :param chunk_size: size of the download chunk\n\n    :return: Absolute path of the downloaded model\n    \"\"\"\n    if model_name not in AVAILABLE_MODELS:\n        logger.error(f\"Invalid model name `{model_name}`, available models are: {AVAILABLE_MODELS}\")\n        return\n    if download_dir is None:\n        download_dir = MODELS_DIR\n        logger.info(f\"No download directory was provided, models will be downloaded to {download_dir}\")\n\n    os.makedirs(download_dir, exist_ok=True)\n\n    url = _get_model_url(model_name=model_name)\n    file_path = Path(download_dir) / os.path.basename(url)\n    # check if the file is already there\n    if file_path.exists():\n        logger.info(f\"Model {model_name} already exists in {download_dir}\")\n    else:\n        # download it from huggingface\n        resp = requests.get(url, stream=True)\n        total = int(resp.headers.get('content-length', 0))\n\n        progress_bar = tqdm(desc=f\"Downloading Model {model_name} ...\",\n                            total=total,\n                            unit='iB',\n                            unit_scale=True,\n                            unit_divisor=1024)\n\n        try:\n            with open(file_path, 'wb') as file, progress_bar:\n                for data in resp.iter_content(chunk_size=chunk_size):\n                    size = file.write(data)\n                    progress_bar.update(size)\n            logger.info(f\"Model downloaded to {file_path.absolute()}\")\n        except Exception as e:\n            # error download, just remove the file\n            os.remove(file_path)\n            raise e\n    return str(file_path.absolute())\n</code></pre>"},{"location":"#pywhispercpp.utils.to_timestamp","title":"to_timestamp","text":"<pre><code>to_timestamp(t, separator=',')\n</code></pre> <p>376 -&gt; 00:00:03,760 1344 -&gt; 00:00:13,440</p> <p>Implementation from <code>whisper.cpp/examples/main</code></p> <p>Parameters:</p> <ul> <li> <code>t</code>               (<code>int</code>)           \u2013            <p>input time from whisper timestamps</p> </li> <li> <code>separator</code>           \u2013            <p>seprator between seconds and milliseconds</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>time representation in hh: mm: ss[separator]ms</p> </li> </ul> Source code in <code>pywhispercpp/utils.py</code> <pre><code>def to_timestamp(t: int, separator=',') -&gt; str:\n    \"\"\"\n    376 -&gt; 00:00:03,760\n    1344 -&gt; 00:00:13,440\n\n    Implementation from `whisper.cpp/examples/main`\n\n    :param t: input time from whisper timestamps\n    :param separator: seprator between seconds and milliseconds\n    :return: time representation in hh: mm: ss[separator]ms\n    \"\"\"\n    # logic exactly from whisper.cpp\n\n    msec = t * 10\n    hr = msec // (1000 * 60 * 60)\n    msec = msec - hr * (1000 * 60 * 60)\n    min = msec // (1000 * 60)\n    msec = msec - min * (1000 * 60)\n    sec = msec // 1000\n    msec = msec - sec * 1000\n    return f\"{int(hr):02,.0f}:{int(min):02,.0f}:{int(sec):02,.0f}{separator}{int(msec):03,.0f}\"\n</code></pre>"},{"location":"#pywhispercpp.utils.output_txt","title":"output_txt","text":"<pre><code>output_txt(segments, output_file_path)\n</code></pre> <p>Creates a raw text from a list of segments</p> <p>Implementation from <code>whisper.cpp/examples/main</code></p> <p>Parameters:</p> <ul> <li> <code>segments</code>               (<code>list</code>)           \u2013            <p>list of segments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>path of the file</p> </li> </ul> Source code in <code>pywhispercpp/utils.py</code> <pre><code>def output_txt(segments: list, output_file_path: str) -&gt; str:\n    \"\"\"\n    Creates a raw text from a list of segments\n\n    Implementation from `whisper.cpp/examples/main`\n\n    :param segments: list of segments\n    :return: path of the file\n    \"\"\"\n    if not output_file_path.endswith('.txt'):\n        output_file_path = output_file_path + '.txt'\n\n    absolute_path = Path(output_file_path).absolute()\n\n    with open(str(absolute_path), 'w') as file:\n        for seg in segments:\n            file.write(seg.text)\n            file.write('\\n')\n    return absolute_path\n</code></pre>"},{"location":"#pywhispercpp.utils.output_vtt","title":"output_vtt","text":"<pre><code>output_vtt(segments, output_file_path)\n</code></pre> <p>Creates a vtt file from a list of segments</p> <p>Implementation from <code>whisper.cpp/examples/main</code></p> <p>Parameters:</p> <ul> <li> <code>segments</code>               (<code>list</code>)           \u2013            <p>list of segments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Absolute path of the file</p> </li> </ul> Source code in <code>pywhispercpp/utils.py</code> <pre><code>def output_vtt(segments: list, output_file_path: str) -&gt; str:\n    \"\"\"\n    Creates a vtt file from a list of segments\n\n    Implementation from `whisper.cpp/examples/main`\n\n    :param segments: list of segments\n    :return: path of the file\n\n    :return: Absolute path of the file\n    \"\"\"\n    if not output_file_path.endswith('.vtt'):\n        output_file_path = output_file_path + '.vtt'\n\n    absolute_path = Path(output_file_path).absolute()\n\n    with open(absolute_path, 'w') as file:\n        file.write(\"WEBVTT\\n\\n\")\n        for seg in segments:\n            file.write(f\"{to_timestamp(seg.t0, separator='.')} --&gt; {to_timestamp(seg.t1, separator='.')}\\n\")\n            file.write(f\"{seg.text}\\n\\n\")\n    return absolute_path\n</code></pre>"},{"location":"#pywhispercpp.utils.output_srt","title":"output_srt","text":"<pre><code>output_srt(segments, output_file_path)\n</code></pre> <p>Creates a srt file from a list of segments</p> <p>Parameters:</p> <ul> <li> <code>segments</code>               (<code>list</code>)           \u2013            <p>list of segments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Absolute path of the file</p> </li> </ul> Source code in <code>pywhispercpp/utils.py</code> <pre><code>def output_srt(segments: list, output_file_path: str) -&gt; str:\n    \"\"\"\n    Creates a srt file from a list of segments\n\n    :param segments: list of segments\n    :return: path of the file\n\n    :return: Absolute path of the file\n    \"\"\"\n    if not output_file_path.endswith('.srt'):\n        output_file_path = output_file_path + '.srt'\n\n    absolute_path = Path(output_file_path).absolute()\n\n    with open(absolute_path, 'w') as file:\n        for i in range(len(segments)):\n            seg = segments[i]\n            file.write(f\"{i+1}\\n\")\n            file.write(f\"{to_timestamp(seg.t0, separator=',')} --&gt; {to_timestamp(seg.t1, separator=',')}\\n\")\n            file.write(f\"{seg.text}\\n\\n\")\n    return absolute_path\n</code></pre>"},{"location":"#pywhispercpp.utils.output_csv","title":"output_csv","text":"<pre><code>output_csv(segments, output_file_path)\n</code></pre> <p>Creates a srt file from a list of segments</p> <p>Parameters:</p> <ul> <li> <code>segments</code>               (<code>list</code>)           \u2013            <p>list of segments</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>str</code>           \u2013            <p>Absolute path of the file</p> </li> </ul> Source code in <code>pywhispercpp/utils.py</code> <pre><code>def output_csv(segments: list, output_file_path: str) -&gt; str:\n    \"\"\"\n    Creates a srt file from a list of segments\n\n    :param segments: list of segments\n    :return: path of the file\n\n    :return: Absolute path of the file\n    \"\"\"\n    if not output_file_path.endswith('.csv'):\n        output_file_path = output_file_path + '.csv'\n\n    absolute_path = Path(output_file_path).absolute()\n\n    with open(absolute_path, 'w') as file:\n        for seg in segments:\n            file.write(f\"{10 * seg.t0}, {10 * seg.t1}, \\\"{seg.text}\\\"\\n\")\n    return absolute_path\n</code></pre>"},{"location":"#pywhispercpp.utils.redirect_stderr","title":"redirect_stderr","text":"<pre><code>redirect_stderr(to=False)\n</code></pre> <p>Redirect stderr to the specified target.</p> <p>Parameters:</p> <ul> <li> <code>to</code>               (<code>bool | TextIO | str | None</code>, default:                   <code>False</code> )           \u2013            <ul> <li>None to suppress output (redirect to devnull), - sys.stdout to redirect to stdout, - A file path (str) to redirect to a file, - False to do nothing (no redirection).</li> </ul> </li> </ul> Source code in <code>pywhispercpp/utils.py</code> <pre><code>@contextlib.contextmanager\ndef redirect_stderr(to: bool | TextIO | str | None = False) -&gt; None:\n    \"\"\"\n    Redirect stderr to the specified target.\n\n    :param to:\n        - None to suppress output (redirect to devnull),\n        - sys.stdout to redirect to stdout,\n        - A file path (str) to redirect to a file,\n        - False to do nothing (no redirection).\n    \"\"\"\n\n    if to is False:\n        # do nothing\n        yield\n        return\n\n    def _resolve_target(target):\n        opened_stream = None\n        if target is None:\n            opened_stream = open(os.devnull, \"w\")\n            return opened_stream, True\n        if isinstance(target, str):\n            opened_stream = open(target, \"w\")\n            return opened_stream, True\n        if hasattr(target, \"write\"):\n            return target, False\n        raise ValueError(\n            \"Invalid `to` parameter; expected None, a filepath string, or a file-like object.\"\n        )\n\n    sys.stderr.flush()\n    try:\n        original_fd = sys.stderr.fileno()\n    except (AttributeError, OSError):\n        # Jupyter or non-standard stderr implementations\n        original_fd = None\n\n    stream, should_close = _resolve_target(to)\n\n    if original_fd is not None and hasattr(stream, \"fileno\"):\n        saved_fd = os.dup(original_fd)\n        try:\n            os.dup2(stream.fileno(), original_fd)\n            yield\n        finally:\n            os.dup2(saved_fd, original_fd)\n            os.close(saved_fd)\n            if should_close:\n                stream.close()\n        return\n\n    # Fallback: Python-level redirect\n    try:\n        with contextlib.redirect_stderr(stream):\n            yield\n    finally:\n        if should_close:\n            stream.close()\n</code></pre>"},{"location":"#pywhispercpp.examples","title":"pywhispercpp.examples","text":""},{"location":"#pywhispercpp.examples.assistant","title":"assistant","text":"<p>A simple example showcasing the use of <code>pywhispercpp</code> as an assistant. The idea is to use a <code>VAD</code> to detect speech (in this example we used webrtcvad), and when speech is detected we run the inference.</p>"},{"location":"#pywhispercpp.examples.assistant.Assistant","title":"Assistant","text":"<pre><code>Assistant(\n    model=\"tiny\",\n    input_device=None,\n    silence_threshold=8,\n    q_threshold=16,\n    block_duration=30,\n    commands_callback=None,\n    **model_params\n)\n</code></pre> <p>Assistant class</p> <p>Example usage <pre><code>from pywhispercpp.examples.assistant import Assistant\n\nmy_assistant = Assistant(commands_callback=print, n_threads=8)\nmy_assistant.start()\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>model</code>           \u2013            <p>whisper.cpp model name or a direct path to a<code>ggml</code> model</p> </li> <li> <code>input_device</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>The input device (aka microphone), keep it None to take the default</p> </li> <li> <code>silence_threshold</code>               (<code>int</code>, default:                   <code>8</code> )           \u2013            <p>The duration of silence after which the inference will be running</p> </li> <li> <code>q_threshold</code>               (<code>int</code>, default:                   <code>16</code> )           \u2013            <p>The inference won't be running until the data queue is having at least <code>q_threshold</code> elements</p> </li> <li> <code>block_duration</code>               (<code>int</code>, default:                   <code>30</code> )           \u2013            <p>minimum time audio updates in ms</p> </li> <li> <code>commands_callback</code>               (<code>Callable[[str], None]</code>, default:                   <code>None</code> )           \u2013            <p>The callback to run when a command is received</p> </li> <li> <code>model_log_level</code>           \u2013            <p>Logging level</p> </li> <li> <code>model_params</code>           \u2013            <p>any other parameter to pass to the whsiper.cpp model see ::: pywhispercpp.constants.PARAMS_SCHEMA</p> </li> </ul> Source code in <code>pywhispercpp/examples/assistant.py</code> <pre><code>def __init__(self,\n             model='tiny',\n             input_device: int = None,\n             silence_threshold: int = 8,\n             q_threshold: int = 16,\n             block_duration: int = 30,\n             commands_callback: Callable[[str], None] = None,\n             **model_params):\n\n    \"\"\"\n    :param model: whisper.cpp model name or a direct path to a`ggml` model\n    :param input_device: The input device (aka microphone), keep it None to take the default\n    :param silence_threshold: The duration of silence after which the inference will be running\n    :param q_threshold: The inference won't be running until the data queue is having at least `q_threshold` elements\n    :param block_duration: minimum time audio updates in ms\n    :param commands_callback: The callback to run when a command is received\n    :param model_log_level: Logging level\n    :param model_params: any other parameter to pass to the whsiper.cpp model see ::: pywhispercpp.constants.PARAMS_SCHEMA\n    \"\"\"\n\n    self.input_device = input_device\n    self.sample_rate = constants.WHISPER_SAMPLE_RATE  # same as whisper.cpp\n    self.channels = 1  # same as whisper.cpp\n    self.block_duration = block_duration\n    self.block_size = int(self.sample_rate * self.block_duration / 1000)\n    self.q = queue.Queue()\n\n    self.vad = webrtcvad.Vad()\n    self.silence_threshold = silence_threshold\n    self.q_threshold = q_threshold\n    self._silence_counter = 0\n\n    self.pwccp_model = Model(model,\n                             print_realtime=False,\n                             print_progress=False,\n                             print_timestamps=False,\n                             single_segment=True,\n                             no_context=True,\n                             **model_params)\n    self.commands_callback = commands_callback\n</code></pre>"},{"location":"#pywhispercpp.examples.assistant.Assistant.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Use this function to start the assistant</p> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> Source code in <code>pywhispercpp/examples/assistant.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"\n    Use this function to start the assistant\n    :return: None\n    \"\"\"\n    logging.info(f\"Starting Assistant ...\")\n    with sd.InputStream(\n            device=self.input_device,  # the default input device\n            channels=self.channels,\n            samplerate=constants.WHISPER_SAMPLE_RATE,\n            blocksize=self.block_size,\n            callback=self._audio_callback):\n\n        try:\n            logging.info(f\"Assistant is listening ... (CTRL+C to stop)\")\n            while True:\n                time.sleep(0.1)\n        except KeyboardInterrupt:\n            logging.info(\"Assistant stopped\")\n</code></pre>"},{"location":"#pywhispercpp.examples.gui","title":"gui","text":""},{"location":"#pywhispercpp.examples.gui.WorkerSignals","title":"WorkerSignals","text":"<p>               Bases: <code>QObject</code></p> <p>Defines signals available from a running worker thread. Supported signals are: - finished: No data - error: tuple (exctype, value, traceback.format_exc()) - result: list (the transcribed segments) - progress: int (0-100) - status_update: str</p>"},{"location":"#pywhispercpp.examples.gui.PyWhisperCppWorker","title":"PyWhisperCppWorker","text":"<pre><code>PyWhisperCppWorker(\n    audio_file_path, model_name, **transcribe_params\n)\n</code></pre> <p>               Bases: <code>Thread</code></p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def __init__(self, audio_file_path, model_name, **transcribe_params):\n    super().__init__()\n    self.audio_file_path = audio_file_path\n    self.model_name = model_name\n    self.transcribe_params = transcribe_params\n    self.signals = WorkerSignals()\n    self._is_running = False\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.PyWhisperCppWorker.run","title":"run","text":"<pre><code>run()\n</code></pre> <p>Executes the transcription process.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def run(self):\n    \"\"\"\n    Executes the transcription process.\n    \"\"\"\n    try:\n        self._is_running = True\n        self.signals.status_update.emit(f\"Loading model: {self.model_name}...\")\n\n        # pywhispercpp will download the specified model if not found\n        model_init_params = {}\n        if 'n_threads' in self.transcribe_params and self.transcribe_params['n_threads'] is not None:\n            model_init_params['n_threads'] = self.transcribe_params['n_threads']\n            # Remove from transcribe_params as it's a model init param\n            del self.transcribe_params['n_threads']\n\n        model = Model(self.model_name, **model_init_params)\n\n        self.signals.status_update.emit(\"Model loaded. Starting transcription...\")\n\n        def new_segment_callback(segment):\n            if not self._is_running:\n                raise RuntimeError(\"Transcription manually stopped\")\n            self.signals.segment.emit(segment)\n\n        segments = model.transcribe(self.audio_file_path,\n                                    new_segment_callback=new_segment_callback,\n                                    progress_callback=lambda progress: self.signals.progress.emit(progress),\n                                    **self.transcribe_params)\n\n        self.signals.status_update.emit(\"Transcription complete!\")\n        self.signals.result.emit(segments)\n\n    except Exception as e:\n        print(e)\n        self.signals.status_update.emit(f\"Error: {str(e)}\")\n        self.signals.error.emit((type(e), e, str(e)))\n    finally:\n        self._is_running = False\n        self.signals.finished.emit()\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp","title":"TranscriptionApp","text":"<pre><code>TranscriptionApp()\n</code></pre> <p>               Bases: <code>QWidget</code></p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def __init__(self):\n    super().__init__()\n    self.selected_file_path = None\n    self.whisper_thread = None\n    # Settings widgets\n    self.model_combo = None\n    self.language_input = None\n    self.translate_checkbox = None\n    self.n_threads_spinbox = None\n    self.no_context_checkbox = None\n    self.temperature_spinbox = None\n    self.settings_content_frame = None  # Frame to hold collapsible settings\n    self.toggle_settings_button = None  # Button to toggle settings\n    self.status_bar_label = None  # New label for the status bar\n    self.about_button = None  # About button\n    self.segments = []  # Store segments for export\n    self.copy_text_button = None  # New button for copy text\n\n    self.initUI()\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.initUI","title":"initUI","text":"<pre><code>initUI()\n</code></pre> <p>Initializes the user interface of the application.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def initUI(self):\n    \"\"\"\n    Initializes the user interface of the application.\n    \"\"\"\n    self.setWindowTitle('PyWhisperCpp Simple GUI')\n    self.setGeometry(100, 100, 450, 500)\n    # Apply the updated stylesheet\n    self.setStyleSheet(STYLESHEET)\n\n    # Main vertical layout\n    main_layout = QVBoxLayout()\n    # Set bottom margin to 0 for the main layout to ensure status bar is flush\n    main_layout.setContentsMargins(4, 4, 4, 0)\n    main_layout.setSpacing(10)\n\n    # --- Header (Title + About Button) ---\n    header_layout = QHBoxLayout()\n    title_label = QLabel(\"PyWhisperCpp Simple GUI\")  # Updated main title label\n    title_label.setObjectName(\"TitleLabel\")  # Add objectName for styling\n    title_label.setAlignment(Qt.AlignLeft)  # Keep title centered within its allocated space\n\n    # Adding stretch before and after title to center it\n    # header_layout.addStretch()\n    header_layout.addWidget(title_label)\n    header_layout.addStretch()\n\n    # About button\n    self.about_button = QPushButton(\"About\")\n    self.about_button.clicked.connect(self.show_about_dialog)\n    # Removed setFixedSize to allow text to fit, or adjust as needed\n    # self.about_button.setFixedSize(50, 25)\n    header_layout.addWidget(self.about_button)  # Add it to the header layout\n\n    main_layout.addLayout(header_layout)  # Add the combined header to main layout\n\n    # --- File Selection Area ---\n    file_frame = QFrame()\n    file_layout = QHBoxLayout(file_frame)\n    file_layout.setContentsMargins(0, 0, 0, 0)\n    file_layout.setSpacing(10)\n\n    self.select_button = QPushButton(\"Select Audio File\")\n    self.select_button.clicked.connect(self.select_file)\n\n    self.file_label = QLabel(\"No file selected.\")\n    self.file_label.setObjectName(\"file_label\")  # Added objectName for styling\n    self.file_label.setSizePolicy(QSizePolicy.Expanding, QSizePolicy.Preferred)\n\n    file_layout.addWidget(self.select_button)\n    file_layout.addWidget(self.file_label)\n    main_layout.addWidget(file_frame)\n\n    # --- Collapsible Settings Section ---\n    settings_group = QGroupBox()  # No title here, using QToolButton for title\n    settings_group_layout = QVBoxLayout(settings_group)\n    settings_group_layout.setContentsMargins(5, 5, 5, 5)\n\n    # Custom title bar for the collapsible group box\n    header_layout_settings = QHBoxLayout()  # Renamed to avoid clash\n    self.toggle_settings_button = QToolButton(settings_group)\n    self.toggle_settings_button.setText(\"Transcription Settings\")\n    self.toggle_settings_button.setToolButtonStyle(Qt.ToolButtonTextBesideIcon)\n    self.toggle_settings_button.setArrowType(Qt.RightArrow)\n    self.toggle_settings_button.setCheckable(True)\n    self.toggle_settings_button.setChecked(False)  # Start collapsed\n    self.toggle_settings_button.clicked.connect(self.toggle_settings_visibility)\n\n    header_layout_settings.addWidget(self.toggle_settings_button)\n    header_layout_settings.addStretch()  # Push button to left\n\n    settings_group_layout.addLayout(header_layout_settings)\n\n    # Frame to hold the actual settings form (this will be hidden/shown)\n    self.settings_content_frame = QFrame()\n    settings_form_layout = QFormLayout(self.settings_content_frame)\n    settings_form_layout.setContentsMargins(15, 5, 10, 10)\n    settings_form_layout.setSpacing(8)\n\n    # Model Selection\n    self.model_combo = QComboBox()\n    self.model_combo.addItems(AVAILABLE_MODELS)\n    self.model_combo.setCurrentText(\"tiny\")  # Default to 'tiny' as requested\n    settings_form_layout.addRow(\"Model:\", self.model_combo)\n\n    # Language Input\n    self.language_input = QLineEdit()\n    self.language_input.setPlaceholderText('e.g., \"en\", \"es\", or leave empty for auto-detect')\n    self.language_input.setText(\"\")  # Default to auto-detect\n    settings_form_layout.addRow(\"Language:\", self.language_input)\n\n    # Translate Checkbox\n    self.translate_checkbox = QCheckBox(\"Translate to English\")\n    self.translate_checkbox.setChecked(False)  # Default\n    settings_form_layout.addRow(\"Translate:\", self.translate_checkbox)\n\n    # N Threads Spinbox\n    self.n_threads_spinbox = QSpinBox()\n    self.n_threads_spinbox.setRange(1, os.cpu_count() if os.cpu_count() else 8)  # Max threads based on CPU cores\n    self.n_threads_spinbox.setValue(4)  # Sensible default\n    settings_form_layout.addRow(\"Number of Threads:\", self.n_threads_spinbox)\n\n    # No Context Checkbox\n    self.no_context_checkbox = QCheckBox(\"No Context (do not use past transcription)\")\n    self.no_context_checkbox.setChecked(False)  # Default\n    settings_form_layout.addRow(\"No Context:\", self.no_context_checkbox)\n\n    # Temperature Spinbox\n    self.temperature_spinbox = QDoubleSpinBox()\n    self.temperature_spinbox.setRange(0.0, 1.0)\n    self.temperature_spinbox.setSingleStep(0.1)\n    self.temperature_spinbox.setValue(0.0)  # Default\n    settings_form_layout.addRow(\"Temperature:\", self.temperature_spinbox)\n\n    settings_group_layout.addWidget(self.settings_content_frame)\n    self.settings_content_frame.setVisible(False)  # Initially hidden\n\n    main_layout.addWidget(settings_group)\n\n    # --- Transcription Button ---\n    self.transcribe_button = QPushButton(\"Transcribe\")\n    self.transcribe_button.setObjectName(\"TranscribeButton\")  # Add objectName for styling\n    self.transcribe_button.setEnabled(False)\n    self.transcribe_button.clicked.connect(self.start_transcription)\n    main_layout.addWidget(self.transcribe_button)\n\n    # --- Stop Button ---\n    self.stop_button = QPushButton(\"Stop\")\n    self.stop_button.setObjectName(\"StopButton\")  # Add objectName for styling\n    self.stop_button.setEnabled(True)\n    self.stop_button.setVisible(False)\n    self.stop_button.clicked.connect(self.stop_transcription)\n    main_layout.addWidget(self.stop_button)\n\n    # --- Progress Bar ---\n    progress_frame = QFrame()\n    progress_layout = QVBoxLayout(progress_frame)\n    progress_layout.setContentsMargins(0, 5, 0, 5)\n    progress_layout.setSpacing(5)\n\n    self.progress_bar = QProgressBar()\n    self.progress_bar.setVisible(False)\n\n    progress_layout.addWidget(self.progress_bar)\n    main_layout.addWidget(progress_frame)\n\n    # --- Transcription Output Table ---\n    output_label = QLabel(\"Transcription Output:\")\n    main_layout.addWidget(output_label)\n\n    self.results_table = QTableWidget()\n    self.results_table.setColumnCount(3)\n    self.results_table.setHorizontalHeaderLabels([\"Start Time\", \"End Time\", \"Text\"])\n    header = self.results_table.horizontalHeader()\n    header.setSectionResizeMode(0, QHeaderView.ResizeToContents)\n    header.setSectionResizeMode(1, QHeaderView.ResizeToContents)\n    header.setSectionResizeMode(2, QHeaderView.Stretch)\n    self.results_table.verticalHeader().setVisible(False)\n    main_layout.addWidget(self.results_table)\n\n    # --- Output Buttons (Export and Copy) ---\n    output_buttons_layout = QHBoxLayout()\n    output_buttons_layout.addStretch()  # Pushes buttons to the right\n\n    # Export Button with Menu\n    self.export_button = QPushButton(\"Export as...\")\n    self.export_button.setEnabled(False)\n    self.export_menu = QMenu(self)\n\n    self.export_action_txt = self.export_menu.addAction(\"Plain Text (.txt)\")\n    self.export_action_srt = self.export_menu.addAction(\"SRT Subtitle (.srt)\")\n    self.export_action_vtt = self.export_menu.addAction(\"VTT Subtitle (.vtt)\")\n    self.export_action_csv = self.export_menu.addAction(\"CSV (.csv)\")\n\n    self.export_action_txt.triggered.connect(lambda: self.export_transcription(\"txt\"))\n    self.export_action_srt.triggered.connect(lambda: self.export_transcription(\"srt\"))\n    self.export_action_vtt.triggered.connect(lambda: self.export_transcription(\"vtt\"))\n    self.export_action_csv.triggered.connect(lambda: self.export_transcription(\"csv\"))\n\n    self.export_button.setMenu(self.export_menu)\n    output_buttons_layout.addWidget(self.export_button)\n\n    # Copy Text Button\n    self.copy_text_button = QPushButton(\"Copy Text\")\n    self.copy_text_button.setEnabled(False)  # Initially disabled\n    self.copy_text_button.clicked.connect(self.copy_all_text_to_clipboard)  # Connect to new method\n    output_buttons_layout.addWidget(self.copy_text_button)\n\n    main_layout.addLayout(output_buttons_layout)\n\n    # --- Status Bar at the very bottom ---\n    self.status_bar_label = QLabel(\"Ready.\")\n    self.status_bar_label.setObjectName(\"status_bar_label\")  # Add objectName for styling\n    self.status_bar_label.setAlignment(Qt.AlignLeft | Qt.AlignVCenter)\n    self.status_bar_label.setContentsMargins(5, 2, 5, 2)\n    main_layout.addWidget(self.status_bar_label)\n\n    self.setLayout(main_layout)\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.toggle_settings_visibility","title":"toggle_settings_visibility","text":"<pre><code>toggle_settings_visibility()\n</code></pre> <p>Toggles the visibility of the settings content frame and updates the arrow.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def toggle_settings_visibility(self):\n    \"\"\"Toggles the visibility of the settings content frame and updates the arrow.\"\"\"\n    is_visible = self.settings_content_frame.isVisible()\n    self.settings_content_frame.setVisible(not is_visible)\n    if not is_visible:\n        self.toggle_settings_button.setArrowType(Qt.DownArrow)\n    else:\n        self.toggle_settings_button.setArrowType(Qt.RightArrow)\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.select_file","title":"select_file","text":"<pre><code>select_file()\n</code></pre> <p>Opens a file dialog to select an audio file.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def select_file(self):\n    \"\"\"\n    Opens a file dialog to select an audio file.\n    \"\"\"\n    options = QFileDialog.Options()\n    file_path, _ = QFileDialog.getOpenFileName(\n        self, \"Select a Media File\", \"\",\n        \"All Files (*)\",\n        options=options\n    )\n    if file_path:\n        self.selected_file_path = file_path\n        self.file_label.setText(f\"Selected: {os.path.basename(file_path)}\")\n        self.transcribe_button.setEnabled(True)\n        self.results_table.setRowCount(0)\n        self.export_button.setEnabled(False)  # Disable export until transcription\n        self.copy_text_button.setEnabled(False)  # Disable copy until transcription\n        self.update_status(\"File selected: \" + os.path.basename(file_path))  # Update new status bar\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.start_transcription","title":"start_transcription","text":"<pre><code>start_transcription()\n</code></pre> <p>Starts the transcription process in a separate thread, passing selected settings.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def start_transcription(self):\n    \"\"\"\n    Starts the transcription process in a separate thread, passing selected settings.\n    \"\"\"\n    if self.selected_file_path:\n        self.transcribe_button.setVisible(False)\n        self.stop_button.setVisible(True)\n        self.select_button.setEnabled(False)\n        self.progress_bar.setVisible(True)\n        self.progress_bar.setValue(0)\n        self.results_table.setRowCount(0)\n        self.export_button.setEnabled(False)  # Disable export during transcription\n        self.copy_text_button.setEnabled(False)  # Disable copy during transcription\n        self.update_status(\"Starting transcription...\")\n        self.segments = []  # Clear segments for new transcription\n\n        # Gather settings from GUI widgets\n        selected_model = self.model_combo.currentText()\n        transcribe_params = {\n            \"language\": self.language_input.text() if self.language_input.text() else None,\n            \"translate\": self.translate_checkbox.isChecked(),\n            \"n_threads\": self.n_threads_spinbox.value(),\n            \"no_context\": self.no_context_checkbox.isChecked(),\n            \"temperature\": self.temperature_spinbox.value(),\n        }\n        # Remove None values to use pywhispercpp defaults where applicable\n        transcribe_params = {k: v for k, v in transcribe_params.items() if v is not None}\n\n        # Create and start the worker thread\n        self.whisper_thread = PyWhisperCppWorker(\n            self.selected_file_path,\n            selected_model,\n            **transcribe_params\n        )\n        self.whisper_thread.signals.result.connect(self.on_transcription_result)\n        self.whisper_thread.signals.segment.connect(self.on_new_segment)\n        self.whisper_thread.signals.finished.connect(self.on_transcription_finished)\n        self.whisper_thread.signals.error.connect(self.on_transcription_error)\n        self.whisper_thread.signals.progress.connect(self.update_progress)\n        self.whisper_thread.signals.status_update.connect(self.update_status)\n        self.whisper_thread.start()\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.format_time","title":"format_time","text":"<pre><code>format_time(milliseconds)\n</code></pre> <p>Converts milliseconds to HH:MM:SS.ms format.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def format_time(self, milliseconds):\n    \"\"\"Converts milliseconds to HH:MM:SS.ms format.\"\"\"\n    seconds_total = milliseconds / 1000\n    minutes, seconds = divmod(seconds_total, 60)\n    hours, minutes = divmod(minutes, 60)\n    return f\"{int(hours):02d}:{int(minutes):02d}:{seconds:06.3f}\"\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.on_transcription_result","title":"on_transcription_result","text":"<pre><code>on_transcription_result(segments)\n</code></pre> <p>Populates the results table with the transcription segments. Stores segments for export.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def on_transcription_result(self, segments):\n    \"\"\"\n    Populates the results table with the transcription segments.\n    Stores segments for export.\n    \"\"\"\n    self.segments = segments  # Store segments\n    self.export_button.setEnabled(True if segments else False)  # Enable export if segments exist\n    self.copy_text_button.setEnabled(True if segments else False)  # Enable copy if segments exist\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.on_transcription_finished","title":"on_transcription_finished","text":"<pre><code>on_transcription_finished()\n</code></pre> <p>Cleans up after the transcription thread is finished.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def on_transcription_finished(self):\n    \"\"\"\n    Cleans up after the transcription thread is finished.\n    \"\"\"\n    self.transcribe_button.setVisible(True)\n    self.transcribe_button.setEnabled(True)\n    self.stop_button.setVisible(False)\n    self.select_button.setEnabled(True)\n    self.progress_bar.setVisible(False)\n    if self.results_table.rowCount() == 0:\n        self.update_status(\"Finished. No transcription data.\")\n    else:\n        self.update_status(\"Transcription finished successfully!\")\n    self.whisper_thread = None\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.on_transcription_error","title":"on_transcription_error","text":"<pre><code>on_transcription_error(err)\n</code></pre> <p>Displays an error message if transcription fails.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def on_transcription_error(self, err):\n    \"\"\"\n    Displays an error message if transcription fails.\n    \"\"\"\n    exctype, value, tb = err\n    error_message = f\"Error: {value}\"\n    self.update_status(error_message)  # Update new status bar\n    self.on_transcription_finished()\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.export_transcription","title":"export_transcription","text":"<pre><code>export_transcription(format_type)\n</code></pre> <p>Handles exporting the transcription to a chosen file format.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def export_transcription(self, format_type):\n    \"\"\"\n    Handles exporting the transcription to a chosen file format.\n    \"\"\"\n    if not self.segments:\n        self.update_status(\"No transcription data to export.\")\n        return\n\n    file_dialog_filter = {\n        \"txt\": \"Plain Text Files (*.txt)\",\n        \"srt\": \"SRT Subtitle Files (*.srt)\",\n        \"vtt\": \"VTT Subtitle Files (*.vtt)\",\n        \"csv\": \"CSV (Comma Separated Values) Files (*.csv)\",\n    }\n\n    default_file_name = os.path.basename(self.selected_file_path).rsplit('.', 1)[\n                            0] + f\".{format_type}\" if self.selected_file_path else f\"transcription.{format_type}\"\n\n    options = QFileDialog.Options()\n    file_path, _ = QFileDialog.getSaveFileName(\n        self, f\"Save Transcription as {format_type.upper()}\",\n        default_file_name,\n        file_dialog_filter.get(format_type, \"All Files (*)\"),\n        options=options\n    )\n\n    if file_path:\n        try:\n            # Use pywhispercpp.utils functions based on format_type\n            if format_type == \"txt\":\n                # For TXT, we'll re-use the text from the table or segments\n                all_text = []\n                for segment in self.segments:\n                    all_text.append(segment.text.strip())\n                output_txt_content = \"\\n\".join(all_text)\n                with open(file_path, 'w', encoding='utf-8') as f:\n                    f.write(output_txt_content)\n\n            elif format_type == \"srt\":\n                if output_srt:\n                    output_srt(self.segments, file_path)\n                else:\n                    raise ImportError(\"pywhispercpp.utils.output_srt not available.\")\n            elif format_type == \"vtt\":\n                if output_vtt:\n                    output_vtt(self.segments, file_path)\n                else:\n                    raise ImportError(\"pywhispercpp.utils.output_vtt not available.\")\n            elif format_type == \"csv\":\n                if output_csv:\n                    # For CSV, we need to pass a list of lists/tuples representing rows\n                    # pywhispercpp.utils.output_csv expects a list of segments and a file path\n                    output_csv(self.segments, file_path)\n                else:\n                    raise ImportError(\"pywhispercpp.utils.output_csv not available.\")\n\n            self.update_status(f\"Transcription successfully exported to {os.path.basename(file_path)}\")\n        except Exception as e:\n            self.update_status(f\"Error exporting to {format_type.upper()}: {e}\")\n    else:\n        self.update_status(\"Export cancelled.\")\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.copy_all_text_to_clipboard","title":"copy_all_text_to_clipboard","text":"<pre><code>copy_all_text_to_clipboard()\n</code></pre> <p>Concatenates all text from segments and copies it to the clipboard.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def copy_all_text_to_clipboard(self):\n    \"\"\"\n    Concatenates all text from segments and copies it to the clipboard.\n    \"\"\"\n    if not self.segments:\n        self.update_status(\"No transcription data to copy.\")\n        return\n\n    all_text = []\n    for segment in self.segments:\n        all_text.append(segment.text.strip())\n\n    QApplication.clipboard().setText(\"\\n\".join(all_text))\n    self.update_status(\"Text copied to clipboard!\")\n</code></pre>"},{"location":"#pywhispercpp.examples.gui.TranscriptionApp.show_about_dialog","title":"show_about_dialog","text":"<pre><code>show_about_dialog()\n</code></pre> <p>Opens a small dialog with About information.</p> Source code in <code>pywhispercpp/examples/gui.py</code> <pre><code>def show_about_dialog(self):\n    \"\"\"Opens a small dialog with About information.\"\"\"\n    about_dialog = QDialog(self)\n    about_dialog.setWindowTitle(\"About PyWhisperCPP Simple GUI\")\n    about_dialog.setFixedSize(400, 220)\n\n    dialog_layout = QVBoxLayout(about_dialog)\n    dialog_layout.setContentsMargins(20, 20, 20, 20)\n\n    info_text = QLabel()\n    info_text.setTextFormat(Qt.RichText)\n    info_text.setText(\n        \"&lt;b&gt;PyWhisperCPP Simple GUI&lt;/b&gt;&lt;br&gt;\"\n        f\"Version {__version__}&lt;br&gt;\"\n        \"&lt;br&gt;\"\n        \"A simple graphical user interface for PyWhisperCpp Using PyQt.&lt;br&gt;&lt;br&gt;\"\n        \"&lt;a href='https://github.com/absadiki/pywhispercpp'&gt;PyWhisperCpp GitHub repository&lt;/a&gt;&lt;br&gt;\"\n        \"&lt;br&gt;\"\n        f\"Copyright \u00a9 {datetime.now().year}\"\n    )\n    info_text.setOpenExternalLinks(True)\n\n    dialog_layout.addWidget(info_text)\n\n    close_button = QPushButton(\"Close\")\n    close_button.clicked.connect(about_dialog.accept)\n    dialog_layout.addWidget(close_button, alignment=Qt.AlignCenter)\n\n    about_dialog.exec_()\n</code></pre>"},{"location":"#pywhispercpp.examples.livestream","title":"livestream","text":"<p>Quick and dirty realtime livestream transcription.</p> <p>Not fully satisfying though :) You are welcome to make it better.</p>"},{"location":"#pywhispercpp.examples.livestream.LiveStream","title":"LiveStream","text":"<pre><code>LiveStream(\n    url,\n    model=\"tiny.en\",\n    block_size=1024,\n    buffer_size=20,\n    sample_size=4,\n    output_device=None,\n    model_log_level=logging.CRITICAL,\n    **model_params\n)\n</code></pre> <p>LiveStream class</p> Note <p>It heavily depends on the machine power, the processor will jump quickly to 100% with the wrong parameters.</p> <p>Example usage <pre><code>from pywhispercpp.examples.livestream import LiveStream\n\nurl = \"\"  # Make sure it is a direct stream URL\nls = LiveStream(url=url, n_threads=4)\nls.start()\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>url</code>           \u2013            <p>Live stream url  <li> <code>model</code>           \u2013            <p>whisper.cpp model</p> </li> <li> <code>block_size</code>               (<code>int</code>, default:                   <code>1024</code> )           \u2013            <p>block size, default to 1024</p> </li> <li> <code>buffer_size</code>               (<code>int</code>, default:                   <code>20</code> )           \u2013            <p>number of blocks used for buffering, default to 20</p> </li> <li> <code>sample_size</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>sample size</p> </li> <li> <code>output_device</code>               (<code>int</code>, default:                   <code>None</code> )           \u2013            <p>the output device, aka the speaker, leave it None to take the default</p> </li> <li> <code>model_log_level</code>           \u2013            <p>logging level</p> </li> <li> <code>model_params</code>           \u2013            <p>any other whisper.cpp params</p> </li> Source code in <code>pywhispercpp/examples/livestream.py</code> <pre><code>def __init__(self,\n             url,\n             model='tiny.en',\n             block_size: int = 1024,\n             buffer_size: int = 20,\n             sample_size: int = 4,\n             output_device: int = None,\n             model_log_level=logging.CRITICAL,\n             **model_params):\n\n    \"\"\"\n    :param url: Live stream url &lt;a direct stream URL&gt;\n    :param model: whisper.cpp model\n    :param block_size: block size, default to 1024\n    :param buffer_size: number of blocks used for buffering, default to 20\n    :param sample_size: sample size\n    :param output_device: the output device, aka the speaker, leave it None to take the default\n    :param model_log_level: logging level\n    :param model_params: any other whisper.cpp params\n    \"\"\"\n    self.url = url\n    self.block_size = block_size\n    self.buffer_size = buffer_size\n    self.sample_size = sample_size\n    self.output_device = output_device\n\n    self.channels = 1\n    self.samplerate = constants.WHISPER_SAMPLE_RATE\n\n    self.q = queue.Queue(maxsize=buffer_size)\n    self.audio_data = np.array([])\n\n    self.pwccp_model = Model(model,\n                             log_level=model_log_level,\n                             print_realtime=True,\n                             print_progress=False,\n                             print_timestamps=False,\n                             single_segment=True,\n                             **model_params)\n</code></pre>"},{"location":"#pywhispercpp.examples.main","title":"main","text":"<p>A simple Command Line Interface to test the package</p>"},{"location":"#pywhispercpp.examples.recording","title":"recording","text":"<p>A simple example showcasing how to use pywhispercpp to transcribe a recording.</p>"},{"location":"#pywhispercpp.examples.recording.Recording","title":"Recording","text":"<pre><code>Recording(duration, model='tiny.en', **model_params)\n</code></pre> <p>Recording class</p> <p>Example usage <pre><code>from pywhispercpp.examples.recording import Recording\n\nmyrec = Recording(5)\nmyrec.start()\n</code></pre></p> Source code in <code>pywhispercpp/examples/recording.py</code> <pre><code>def __init__(self,\n             duration: int,\n             model: str = 'tiny.en',\n             **model_params):\n    self.duration = duration\n    self.sample_rate = pywhispercpp.constants.WHISPER_SAMPLE_RATE\n    self.channels = 1\n    self.pwcpp_model = Model(model, print_realtime=True, **model_params)\n</code></pre>"}]}